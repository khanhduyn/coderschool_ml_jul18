{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 7\n",
    "\n",
    "We will be examining unsupervised learning and addressing large data sets.\n",
    "\n",
    "**Goal**\n",
    "\n",
    "After this lesson, you should:\n",
    "1. Understand *supervised* and *unsupervised* learning\n",
    "1. Understand and implement the *k-means clustering* unsupervised learning algorithm.\n",
    "1. Understand *bag-of-words*, a basic NLP technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn as skl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sea\n",
    "\n",
    "# control the plotsize\n",
    "plt.rcParams['figure.figsize'] = [10,5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Exam Structure\n",
    "\n",
    "We've discussed the final exam, but I wanted to make it more concrete. You will be presenting a notebook that covers an analysis of a chosen data set. You can find examples of notebooks on [kaggle.com](http://www.kaggle.com), where they call the notebooks \"kernels\".\n",
    "\n",
    "You will present *for a maximum* of 5 minutes. During that 5 minutes, you should cover:\n",
    "\n",
    "1. What is the data set about?\n",
    "    - what data are you analyzing, and what do you hope to achieve by analyzing it?\n",
    "1. What data exploration techniques did you use?\n",
    "    - Please refer to specific techniques covered in the course.\n",
    "    - You must have some exploratory graphs which help us visualize the data.\n",
    "1. Which models did you employ?\n",
    "    - How did the models perform against one-another?\n",
    "    - What hyperparameters did you adjust?\n",
    "    - How did regularization affect the results?\n",
    "1. What verification methodologies did you use?\n",
    "    - talk about the recall and precision of your analysis\n",
    "    - talk about any specific cross validation methods you used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised vs Unsupervised Learning\n",
    "\n",
    "So far in this course, we have dealt with models that require manual feature labeling, as well as a predetermined predictor. In other words, we know what the output values of our samples should be when implementing our models.\n",
    "\n",
    "In unsupervised learning, we don't have output values in mind when building our algorithm. The point of unsupervised learning is to model the underlying distribution of our samples and to determine the structure of our data.\n",
    "\n",
    "In general, unsupervised learning algorithms can be grouped into two categories:\n",
    "\n",
    "1. **clustering** is similar to classification, in the sense that we want to determine the inherent groupings in our data\n",
    "1. **association** is the problem of discovering certain rules that govern our data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way we can think about unsupervised and supervised learning is to consider the difference between drawing *boundaries* and *grouping* samples.\n",
    "\n",
    "<img src=\"https://deepcast.ai/static/img/article3/art3-fig1.png\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In other words:\n",
    "- supervised learning deals with labeled data, and the algorithm attempts to predict the output\n",
    "- unsupervised learning deals with unlabeled data, and the algorithm learns the structure of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $k$-means clustering\n",
    "\n",
    "A simple, but effective, unsupervised learning algorithm is **$k$-means clustering**. This algorithm takes *unlabeled data* and attempts to find groups in the data. The number of groups it attempts to find is represented by $k$.\n",
    "\n",
    "Some examples of how we might use $k$-means clustering:\n",
    "- take a large set of documents and classify them based on topic, content, or metadata\n",
    "- identify geographic clusters against a category, such as identifying higher risk crime areas\n",
    "- detect phishing attacks and common patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The algorithm\n",
    "\n",
    "1. randomly define $k$ centroids (we can think of a centroid as circle)\n",
    "1. for each sample point closest to the corresponding centroid, find the distance between the point and the center of the centroids\n",
    "1. assign the sample point to the centroid which is \"nearest\" (i.e. find the cluster whose mean has the least error)\n",
    "1. update the centroids by calculating new means of all the values belonging to that centroid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/e/ea/K-means_convergence.gif\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########\n",
    "# Let's use k-means clustering to identify clusters in some random data\n",
    "########\n",
    "def build_data(k):\n",
    "    \"\"\" Builds fake data for our k-means algorithm.\n",
    "        k - the maximum number of \"centers\" possible\n",
    "    \"\"\"\n",
    "    if k < 1:\n",
    "        print(\"ERROR: Must provide at least one center with which to cluster data.\")\n",
    "        return\n",
    "    clusters = []\n",
    "    centers = [np.array([np.random.randint(0,k*5),np.random.randint(0,k*5)]) for center in range(np.random.randint(1,k))]\n",
    "    for center in centers:\n",
    "        samples = 300\n",
    "        clusters.append(np.random.randn(samples,2) + center)\n",
    "\n",
    "    data = np.concatenate(clusters, axis=0)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = build_data(5)\n",
    "plt.scatter(raw[:,0],raw[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's use sklearn's k-means algorithm\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "np.random.shuffle(raw)\n",
    "\n",
    "split = 4/5\n",
    "train = raw[:int(split*len(raw))]\n",
    "test = raw[int(split*len(raw)):]\n",
    "\n",
    "kmeans = KMeans(n_clusters=3)\n",
    "kmeans.fit(train)\n",
    "\n",
    "# retreive our labels\n",
    "predicted = kmeans.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_colors = ['b','g','r','c','m','y','k'][:max(predicted) + 1]\n",
    "colors = [possible_colors[d] for d in predicted]\n",
    "\n",
    "plt.scatter(test[:,0],test[:,1],c=colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimal $k$ in $k$-means?\n",
    "\n",
    "It's often not very apparent what $k$ should be. So how do we validate the most appropriate $k$?\n",
    "\n",
    "We can use the **elbow** method, which runs $k$-means for a range of possible $k$ values, and for each $k$, we score the result. A common scoring is to calculate the sum of squared errors.\n",
    "\n",
    "We plot the scores and consider the \"elbow\" value to be the optimal $k$. The intuition is that we want to minimize our sum of squared error (sse), but the sse $\\rightarrow 0$ as $k \\rightarrow \\infty$. So the goal is to minimize $k$ while also choosing a small sse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# possible ks\n",
    "k_values = range(1,10)\n",
    "k_mean_models = [KMeans(n_clusters=k) for k in k_values]\n",
    "scores = [kmean.fit(train).score(train) for kmean in k_mean_models]\n",
    "\n",
    "norm_scores = [score/max(scores) for score in scores]\n",
    "\n",
    "plt.plot(norm_scores)\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Disadvantages\n",
    "While it's very easy to understand, trains fast, and can be widely applied to cluster analysis, there are some big disadvantages:\n",
    "\n",
    "1. the performance of the algorithm is slower than the other clustering algorithms\n",
    "1. clusters are assumed to approximately \"spherical\" and evenly sized\n",
    "1. small variations can lead to extremely different results (high variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For instance, consider this data set\n",
    "import math\n",
    "\n",
    "cluster_1 = np.random.randn(300,2) + np.array([0,0])\n",
    "cluster_2 = []\n",
    "\n",
    "def perturb():\n",
    "    return np.random.randint(0,2) * np.random.random_sample()\n",
    "\n",
    "for x in range(500):\n",
    "    cluster_2.append([8*math.cos(x) + perturb(), 8*math.sin(x) + perturb()])\n",
    "        \n",
    "cluster_2 = np.array(cluster_2)\n",
    "data = np.concatenate((cluster_1,cluster_2),axis=0)\n",
    "\n",
    "plt.scatter(data[:,0],data[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = data\n",
    "np.random.shuffle(raw)\n",
    "\n",
    "split = 4/5\n",
    "train = raw[:int(split*len(raw))]\n",
    "test = raw[int(split*len(raw)):]\n",
    "\n",
    "kmeans = KMeans(n_clusters=2)\n",
    "kmeans.fit(train)\n",
    "predicted = kmeans.predict(test)\n",
    "\n",
    "possible_colors = ['b','g','r','c','m','y','k'][:max(predicted) + 1]\n",
    "colors = [possible_colors[d] for d in predicted]\n",
    "\n",
    "plt.scatter(test[:,0],test[:,1],c=colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural Language Processing (NLP)\n",
    "\n",
    "NLP refers to the practice of performing predictive and classification analysis against natural language. This is often a difficult problem because language is inherently messy.\n",
    "\n",
    "There are a number of NLP business applications, and this is a field which is readily emerging at the forefront of new ventures:\n",
    "\n",
    "1. automated customer service\n",
    "1. sentiment analysis\n",
    "1. market intelligence and competitive analysis\n",
    "1. personal assitants (e.g. siri, google voice, alexa, etc...)\n",
    "\n",
    "As you've seen, most of our ML models work with numbers, and so we need to figure out a way to model language using numbers. Specifically, we will be looking at vectors that represent our natural language. This is a process known as **feature encoding**.\n",
    " \n",
    " **bag-of-words** is a simple method that allows us to extract features from text in order to build useful models. In order to develop this feature extraction technique, we will need two ideas:\n",
    " \n",
    " 1. a dictionary of known words\n",
    " 1. a way to measure the frequency of known words\n",
    " \n",
    " In the same way that a set discards ordering information, bag-of-words discards all information about the structure and ordering of the words. It just considers the words themselves.\n",
    "\n",
    "Let's examine the bag-of-words approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider this vietnamese nursery rhyme:\n",
    "\n",
    "```\n",
    "Con cò bé bé\n",
    "Nó đậu cành tre\n",
    "Đi không hỏi mẹ\n",
    "Biết đi đường nào\n",
    "Khi đi em hỏi\n",
    "Khi về em chào\n",
    "Miệng em chúm chím\n",
    "Mẹ có yêu không nào\n",
    "```\n",
    "\n",
    "Translation\n",
    "```\n",
    "A little baby stork\n",
    "It's perched on a branch of bamboo \n",
    "It went away, but didn't ask its mother.\n",
    "How does she know where it went?\n",
    "When you go out, ask,\n",
    "When you arrive home, say hello.\n",
    "Your mouth is slightly open*\n",
    "How can mother not love you.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we build our dictionary, which consists of all the words in the rhyme\n",
    "# This allows us to design the vocabulary we will be using in our analysis\n",
    "\n",
    "poem = \"\"\"Con cò bé bé\n",
    "Nó đậu cành tre\n",
    "Đi không hỏi mẹ\n",
    "Biết đi đường nào\n",
    "Khi đi em hỏi\n",
    "Khi về em chào\n",
    "Miệng em chúm chím\n",
    "Mẹ có yêu không nào\"\"\"\n",
    "\n",
    "unique_words = set()\n",
    "for word in poem.replace('\\n',' ').split(' '):\n",
    "    unique_words.add(word.lower())\n",
    "\n",
    "vocabulary = [word for word in unique_words]\n",
    "print(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we turn each line into a vector representation of our vocabulary.\n",
    "# In this example, we will consider each line to be a single \"document\"\n",
    "\n",
    "lines = poem.split('\\n')\n",
    "\n",
    "def extract_feature(document):\n",
    "    document_vector = []\n",
    "    dsplit = set(map(lambda x: x.lower(), document.split(' ')))\n",
    "    for word in vocabulary:\n",
    "        if (word in dsplit):\n",
    "            document_vector.append(1)\n",
    "        else:\n",
    "            document_vector.append(0)\n",
    "    return document_vector\n",
    "\n",
    "encoded_document = []\n",
    "for line in lines:\n",
    "    encoded_document.append(extract_feature(line))\n",
    "\n",
    "encoded_document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can anyone make some statements about the problems associated with encoding a document as above? What might be some ways we could mitigate those problems?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grams\n",
    "\n",
    "One way to deal with the sparse vector problem is to consider $n-$grams of words. Specifically, an **$n-$gram** is an **$n-$token** sequence of words. For example, a $2-$gram (aka bigram) might be a sequence such as \"it was\", \"put down\", \"your sandwich\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's build all the bigrams for the first document from the above poem\n",
    "\n",
    "def build_n_grams(document, n):\n",
    "    \"\"\"Constructs a vector of n_grams for a given document\n",
    "    \"\"\"\n",
    "    document_vector = []\n",
    "    return document_vector"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
