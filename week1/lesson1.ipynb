{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 1 - Fundamentals (part 1)\n",
    "\n",
    "**Goal**\n",
    "After this first lesson, you should:\n",
    "1. Know how to setup a working python environment on your computer using anaconda\n",
    "2. Understand the following concepts:\n",
    "    - variance\n",
    "    - bias\n",
    "    - overfitting\n",
    "    - underfitting\n",
    "-----\n",
    "\n",
    "### Setting up Python\n",
    "\n",
    "If you have not already setup a python environment, you should do so know:\n",
    "\n",
    "**MacOs installation**\n",
    "https://conda.io/docs/user-guide/install/macos.html\n",
    "\n",
    "**Windows installation**\n",
    "https://conda.io/docs/user-guide/install/windows.html\n",
    "\n",
    "You can test that you succeeded by opening a terminal prompt and typing:\n",
    "\n",
    "    jupyter notebook\n",
    "    \n",
    "Success will open a new tab in your default browser which has a notebook that looks like this one\n",
    "\n",
    "### Understanding Numpy and Pandas\n",
    "\n",
    "[numpy](http://www.numpy.org/) and [pandas](https://pandas.pydata.org/) are libraries which help manipulate data in a structured manner. These libraries provide datastructures and algorithms that help manipulate data.\n",
    "\n",
    "Since we installed numpy and pandas using anaconda and the [conda package manager](https://conda.io/docs/index.html), we have access to all the libraries we will need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we always need to include our libraries\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3 4]\n",
      "[[1 2]\n",
      " [3 4]]\n"
     ]
    }
   ],
   "source": [
    "# Let's learn some basic operations with the numpy library\n",
    "########################\n",
    "\n",
    "# Create a 1D array\n",
    "a = np.array([1,2,3,4])\n",
    "\n",
    "# Create a 2D matrix\n",
    "b = np.matrix([[1,2],[3,4]])\n",
    "\n",
    "# Create a 2D identity matrix with 4 rows\n",
    "c = np.eye(4)\n",
    "\n",
    "# Scalar multiplication\n",
    "5 * a\n",
    "\n",
    "# matrix multiplication\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try and perform the following matrix calculation using numpy.\n",
    "\n",
    "Calculate $A$, where $\n",
    "A = \n",
    "   \\begin{bmatrix} \n",
    "      0 & 1 \\\\\n",
    "      1 & 0 \\\\ \n",
    "   \\end{bmatrix}\n",
    "   \\begin{bmatrix}\n",
    "      3 \\\\\n",
    "      4 \\\\\n",
    "   \\end{bmatrix}\n",
    "$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4 3]]\n"
     ]
    }
   ],
   "source": [
    "# Answer\n",
    "b = [[0, 1],\n",
    "     [1, 0]]\n",
    "c = [3, 4]\n",
    "A = np.matrix(b).dot(np.array(c))\n",
    "# A = np.matrix(b) * np.array(c)\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are many algorithms which numpy provides.\n",
    "######################\n",
    "\n",
    "# Take the dot product of two arrays\n",
    "arr1 = np.array([1,2,3])\n",
    "arr2 = np.array([4,5,6])\n",
    "\n",
    "np.dot(arr1,arr2)\n",
    "\n",
    "# Generate an array from a specified range\n",
    "arr3 = np.arange(10)\n",
    "\n",
    "# Shuffle an array\n",
    "np.random.shuffle(arr3)\n",
    "\n",
    "# Generate a random integer\n",
    "r1 = np.random.randint(1,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can discover many more algorithms that numpy supports here:\n",
    "https://docs.scipy.org/doc/numpy-1.13.0/reference/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Application and Review\n",
    "\n",
    "> Let's use numpy to simulate coin flips and dice rolls.\n",
    "\n",
    "Assume you flip 5 fair coins (i.e. the probability of a coin landing on one side is 0.5). Write a small function which will determine the _estimated probability_ that at most 3 coins will be heads or exactly 1 coin lands on heads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.78038"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def flip_coins():\n",
    "    trials = 100000\n",
    "    results = 0\n",
    "    for i in range(0, trials):\n",
    "        each_try = np.random.randint(0, 2, size=5)\n",
    "        if sum(each_try) <= 3 and sum(each_try) > 0:\n",
    "            results += 1\n",
    "    return results/trials\n",
    "flip_coins()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.90113"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Challenge problem\n",
    "# Write a function which takes N dice and returns the estimated probability\n",
    "# that the total score is greater than 11 or odd\n",
    "\n",
    "## WRITE YOUR SOLUTION HERE\n",
    "def roll_dices(N):\n",
    "    trials = 100000\n",
    "    result = 0\n",
    "    for i in range(trials):\n",
    "        rolling = np.random.randint(1, 7, size=N)\n",
    "        sum_rolling = sum(rolling)\n",
    "        if (sum_rolling > 11) or (sum_rolling % 2):\n",
    "            result += 1\n",
    "    \n",
    "    return result/trials\n",
    "roll_dices(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variance\n",
    "\n",
    "Given a set of data, we can use a few techinques to determine the \"shape\" of the data.\n",
    "\n",
    "Assume we have the following set of data $X = \\{x_1, x_2, x_3, ..., x_n\\}$.\n",
    "\n",
    "> **arithmetic mean**\n",
    "The arithmetic mean is defined as $\\overline{X} = \\frac{1}{n}(x_1 + x_2 + x_3 + ... + x_n)$\n",
    "    \n",
    "> **median**\n",
    "The median is defined as the $M = x_{\\frac{n}{2}}$ if $n$ is odd. Otherwise, we take the average of the middle two numbers in an _ordered_ set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.676880229460275e-05\n",
      "2.0471111814741962e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 1.0000e+00, 1.0000e+00, 0.0000e+00,\n",
       "        1.0000e+00, 0.0000e+00, 1.0000e+00, 2.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00,\n",
       "        1.0000e+00, 1.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        1.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 2.0000e+00,\n",
       "        1.0000e+00, 0.0000e+00, 0.0000e+00, 2.0000e+00, 1.0000e+00,\n",
       "        1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.0000e+00,\n",
       "        1.0000e+00, 1.0000e+00, 1.0000e+00, 4.0000e+00, 2.0000e+00,\n",
       "        6.0000e+00, 6.0000e+00, 3.0000e+00, 1.0000e+00, 3.0000e+00,\n",
       "        2.0000e+00, 1.0000e+00, 3.0000e+00, 4.0000e+00, 9.0000e+00,\n",
       "        7.0000e+00, 3.0000e+00, 6.0000e+00, 5.0000e+00, 7.0000e+00,\n",
       "        6.0000e+00, 2.0000e+00, 2.0000e+00, 7.0000e+00, 4.0000e+00,\n",
       "        8.0000e+00, 5.0000e+00, 7.0000e+00, 7.0000e+00, 1.4000e+01,\n",
       "        8.0000e+00, 6.0000e+00, 8.0000e+00, 9.0000e+00, 1.6000e+01,\n",
       "        1.0000e+01, 9.0000e+00, 1.0000e+01, 1.8000e+01, 1.3000e+01,\n",
       "        1.4000e+01, 1.9000e+01, 1.9000e+01, 1.8000e+01, 3.2000e+01,\n",
       "        1.6000e+01, 1.4000e+01, 1.6000e+01, 2.0000e+01, 2.5000e+01,\n",
       "        2.1000e+01, 2.8000e+01, 2.6000e+01, 2.9000e+01, 2.1000e+01,\n",
       "        2.8000e+01, 3.4000e+01, 2.1000e+01, 2.9000e+01, 3.7000e+01,\n",
       "        4.2000e+01, 3.8000e+01, 2.8000e+01, 3.4000e+01, 3.8000e+01,\n",
       "        4.4000e+01, 3.6000e+01, 5.5000e+01, 4.3000e+01, 5.7000e+01,\n",
       "        6.3000e+01, 5.6000e+01, 5.8000e+01, 6.3000e+01, 6.4000e+01,\n",
       "        7.9000e+01, 5.5000e+01, 8.3000e+01, 8.3000e+01, 7.7000e+01,\n",
       "        6.9000e+01, 7.2000e+01, 1.0500e+02, 9.0000e+01, 9.2000e+01,\n",
       "        9.5000e+01, 1.3000e+02, 9.7000e+01, 1.2200e+02, 9.5000e+01,\n",
       "        1.1600e+02, 1.1100e+02, 1.2600e+02, 1.3500e+02, 1.3100e+02,\n",
       "        1.4000e+02, 1.2200e+02, 1.5400e+02, 1.5300e+02, 1.5300e+02,\n",
       "        1.7100e+02, 1.9600e+02, 1.9100e+02, 1.9100e+02, 2.1800e+02,\n",
       "        2.2600e+02, 2.1300e+02, 2.2200e+02, 2.4100e+02, 2.3000e+02,\n",
       "        2.7700e+02, 2.9000e+02, 2.9400e+02, 2.9600e+02, 2.8300e+02,\n",
       "        2.6900e+02, 3.2600e+02, 3.4200e+02, 3.2800e+02, 3.7300e+02,\n",
       "        3.4000e+02, 3.7100e+02, 3.8500e+02, 3.8400e+02, 4.0100e+02,\n",
       "        4.3900e+02, 4.4600e+02, 4.0400e+02, 4.9100e+02, 4.8800e+02,\n",
       "        5.3500e+02, 5.1700e+02, 5.3200e+02, 5.5800e+02, 5.7100e+02,\n",
       "        5.5600e+02, 6.1700e+02, 6.3200e+02, 6.5100e+02, 6.8000e+02,\n",
       "        7.1800e+02, 6.9200e+02, 7.1400e+02, 7.4700e+02, 7.6600e+02,\n",
       "        8.4000e+02, 8.8300e+02, 8.4600e+02, 9.0400e+02, 9.5700e+02,\n",
       "        9.6800e+02, 9.5600e+02, 1.0730e+03, 1.0540e+03, 1.1090e+03,\n",
       "        1.1710e+03, 1.1430e+03, 1.2030e+03, 1.1580e+03, 1.2920e+03,\n",
       "        1.3260e+03, 1.3320e+03, 1.3830e+03, 1.3570e+03, 1.5160e+03,\n",
       "        1.5520e+03, 1.5440e+03, 1.5850e+03, 1.6220e+03, 1.6710e+03,\n",
       "        1.7210e+03, 1.7640e+03, 1.8980e+03, 1.9540e+03, 1.9680e+03,\n",
       "        2.0390e+03, 2.0990e+03, 2.1380e+03, 2.1760e+03, 2.2240e+03,\n",
       "        2.2720e+03, 2.3770e+03, 2.4910e+03, 2.5010e+03, 2.5410e+03,\n",
       "        2.6740e+03, 2.7400e+03, 2.7250e+03, 2.9000e+03, 2.9320e+03,\n",
       "        3.0370e+03, 3.1120e+03, 3.1420e+03, 3.2530e+03, 3.4130e+03,\n",
       "        3.5310e+03, 3.5470e+03, 3.6440e+03, 3.6310e+03, 3.7700e+03,\n",
       "        3.8480e+03, 4.0760e+03, 4.1160e+03, 4.2790e+03, 4.4130e+03,\n",
       "        4.4930e+03, 4.5040e+03, 4.7250e+03, 4.6680e+03, 4.9670e+03,\n",
       "        4.9580e+03, 5.1580e+03, 5.1860e+03, 5.2760e+03, 5.5160e+03,\n",
       "        5.5700e+03, 5.7730e+03, 5.9220e+03, 6.0350e+03, 6.0150e+03,\n",
       "        6.1830e+03, 6.4510e+03, 6.5090e+03, 6.8370e+03, 6.8320e+03,\n",
       "        6.9730e+03, 7.1510e+03, 7.2110e+03, 7.4820e+03, 7.6520e+03,\n",
       "        7.7670e+03, 8.1050e+03, 8.2910e+03, 8.3420e+03, 8.5050e+03,\n",
       "        8.6700e+03, 8.7320e+03, 9.0430e+03, 9.1430e+03, 9.3330e+03,\n",
       "        9.6900e+03, 9.8680e+03, 9.9350e+03, 1.0056e+04, 1.0285e+04,\n",
       "        1.0532e+04, 1.0899e+04, 1.0871e+04, 1.1174e+04, 1.1297e+04,\n",
       "        1.1431e+04, 1.1758e+04, 1.2059e+04, 1.2271e+04, 1.2418e+04,\n",
       "        1.2642e+04, 1.2915e+04, 1.3291e+04, 1.3456e+04, 1.3609e+04,\n",
       "        1.3964e+04, 1.4174e+04, 1.4275e+04, 1.4503e+04, 1.4868e+04,\n",
       "        1.5055e+04, 1.5250e+04, 1.5636e+04, 1.5757e+04, 1.6139e+04,\n",
       "        1.6427e+04, 1.6853e+04, 1.6846e+04, 1.7138e+04, 1.7584e+04,\n",
       "        1.7756e+04, 1.8034e+04, 1.8109e+04, 1.8590e+04, 1.8716e+04,\n",
       "        1.8886e+04, 1.9466e+04, 1.9815e+04, 1.9871e+04, 2.0452e+04,\n",
       "        2.0571e+04, 2.0731e+04, 2.0977e+04, 2.1285e+04, 2.1659e+04,\n",
       "        2.1677e+04, 2.2329e+04, 2.2711e+04, 2.2836e+04, 2.3120e+04,\n",
       "        2.3659e+04, 2.4068e+04, 2.4180e+04, 2.4505e+04, 2.4865e+04,\n",
       "        2.5489e+04, 2.5466e+04, 2.5763e+04, 2.6000e+04, 2.6188e+04,\n",
       "        2.6383e+04, 2.7148e+04, 2.6865e+04, 2.7316e+04, 2.7720e+04,\n",
       "        2.8216e+04, 2.8033e+04, 2.8762e+04, 2.9086e+04, 2.9026e+04,\n",
       "        2.9940e+04, 3.0110e+04, 3.0562e+04, 3.0729e+04, 3.0750e+04,\n",
       "        3.1465e+04, 3.1303e+04, 3.1632e+04, 3.2227e+04, 3.2436e+04,\n",
       "        3.2910e+04, 3.3012e+04, 3.3199e+04, 3.3537e+04, 3.3941e+04,\n",
       "        3.4262e+04, 3.4476e+04, 3.4777e+04, 3.5290e+04, 3.5302e+04,\n",
       "        3.5823e+04, 3.5888e+04, 3.5923e+04, 3.6488e+04, 3.6499e+04,\n",
       "        3.6852e+04, 3.6945e+04, 3.7588e+04, 3.7210e+04, 3.7634e+04,\n",
       "        3.8159e+04, 3.8292e+04, 3.8389e+04, 3.8836e+04, 3.8913e+04,\n",
       "        3.9501e+04, 3.9553e+04, 3.9793e+04, 4.0223e+04, 4.0049e+04,\n",
       "        4.0350e+04, 4.0759e+04, 4.1399e+04, 4.0881e+04, 4.0931e+04,\n",
       "        4.1368e+04, 4.1628e+04, 4.1944e+04, 4.2260e+04, 4.1999e+04,\n",
       "        4.2343e+04, 4.2434e+04, 4.2562e+04, 4.2181e+04, 4.2662e+04,\n",
       "        4.3513e+04, 4.3012e+04, 4.3353e+04, 4.3422e+04, 4.3671e+04,\n",
       "        4.3809e+04, 4.3406e+04, 4.3662e+04, 4.4116e+04, 4.4205e+04,\n",
       "        4.4255e+04, 4.4370e+04, 4.4702e+04, 4.4613e+04, 4.4412e+04,\n",
       "        4.4562e+04, 4.4755e+04, 4.4563e+04, 4.5337e+04, 4.5015e+04,\n",
       "        4.5125e+04, 4.5026e+04, 4.4790e+04, 4.5155e+04, 4.5271e+04,\n",
       "        4.4676e+04, 4.5063e+04, 4.4688e+04, 4.4796e+04, 4.4798e+04,\n",
       "        4.5045e+04, 4.5067e+04, 4.4804e+04, 4.4858e+04, 4.4810e+04,\n",
       "        4.4692e+04, 4.4487e+04, 4.4661e+04, 4.5028e+04, 4.4472e+04,\n",
       "        4.4278e+04, 4.4385e+04, 4.4077e+04, 4.3874e+04, 4.4090e+04,\n",
       "        4.3964e+04, 4.3558e+04, 4.3777e+04, 4.3453e+04, 4.3652e+04,\n",
       "        4.3290e+04, 4.3545e+04, 4.3344e+04, 4.2983e+04, 4.2874e+04,\n",
       "        4.2551e+04, 4.2263e+04, 4.2326e+04, 4.1956e+04, 4.1716e+04,\n",
       "        4.1864e+04, 4.1647e+04, 4.1411e+04, 4.0994e+04, 4.1155e+04,\n",
       "        4.0643e+04, 4.0882e+04, 4.0488e+04, 4.0338e+04, 4.0069e+04,\n",
       "        3.9581e+04, 3.9698e+04, 3.9494e+04, 3.9290e+04, 3.8916e+04,\n",
       "        3.8747e+04, 3.8385e+04, 3.8326e+04, 3.7611e+04, 3.7267e+04,\n",
       "        3.7476e+04, 3.7058e+04, 3.6868e+04, 3.6523e+04, 3.6506e+04,\n",
       "        3.5871e+04, 3.5733e+04, 3.5133e+04, 3.5115e+04, 3.5173e+04,\n",
       "        3.4436e+04, 3.4743e+04, 3.4032e+04, 3.4102e+04, 3.3532e+04,\n",
       "        3.3336e+04, 3.3243e+04, 3.2800e+04, 3.2103e+04, 3.2157e+04,\n",
       "        3.1929e+04, 3.1552e+04, 3.1415e+04, 3.0875e+04, 3.0388e+04,\n",
       "        3.0629e+04, 2.9690e+04, 2.9323e+04, 2.9590e+04, 2.9193e+04,\n",
       "        2.8798e+04, 2.8183e+04, 2.7716e+04, 2.7771e+04, 2.7306e+04,\n",
       "        2.7195e+04, 2.6772e+04, 2.6395e+04, 2.6276e+04, 2.6057e+04,\n",
       "        2.5675e+04, 2.5256e+04, 2.5250e+04, 2.4721e+04, 2.4529e+04,\n",
       "        2.4346e+04, 2.3739e+04, 2.3627e+04, 2.3167e+04, 2.2784e+04,\n",
       "        2.2746e+04, 2.2347e+04, 2.2046e+04, 2.2035e+04, 2.1526e+04,\n",
       "        2.1265e+04, 2.0826e+04, 2.0629e+04, 2.0321e+04, 1.9872e+04,\n",
       "        1.9605e+04, 1.9282e+04, 1.9248e+04, 1.8810e+04, 1.8431e+04,\n",
       "        1.8363e+04, 1.7744e+04, 1.7684e+04, 1.7637e+04, 1.7223e+04,\n",
       "        1.6613e+04, 1.6575e+04, 1.6436e+04, 1.6215e+04, 1.6015e+04,\n",
       "        1.5761e+04, 1.5310e+04, 1.5215e+04, 1.4627e+04, 1.4830e+04,\n",
       "        1.4449e+04, 1.4009e+04, 1.3758e+04, 1.3434e+04, 1.3449e+04,\n",
       "        1.3074e+04, 1.2852e+04, 1.2829e+04, 1.2626e+04, 1.2134e+04,\n",
       "        1.2099e+04, 1.1897e+04, 1.1585e+04, 1.1099e+04, 1.1213e+04,\n",
       "        1.0917e+04, 1.0700e+04, 1.0730e+04, 1.0522e+04, 1.0200e+04,\n",
       "        9.9150e+03, 9.7180e+03, 9.5890e+03, 9.3670e+03, 9.2910e+03,\n",
       "        8.9160e+03, 8.7890e+03, 8.6670e+03, 8.4990e+03, 8.2550e+03,\n",
       "        8.1850e+03, 7.9610e+03, 7.7370e+03, 7.6970e+03, 7.5540e+03,\n",
       "        7.3420e+03, 7.2520e+03, 6.9530e+03, 6.7350e+03, 6.6540e+03,\n",
       "        6.4770e+03, 6.5310e+03, 6.3000e+03, 5.9570e+03, 6.1520e+03,\n",
       "        5.8750e+03, 5.7530e+03, 5.4990e+03, 5.4520e+03, 5.4040e+03,\n",
       "        5.1030e+03, 5.0900e+03, 4.9760e+03, 4.9570e+03, 4.6550e+03,\n",
       "        4.6240e+03, 4.5230e+03, 4.5390e+03, 4.3740e+03, 4.2170e+03,\n",
       "        4.1350e+03, 4.0440e+03, 3.9140e+03, 3.7480e+03, 3.7770e+03,\n",
       "        3.7370e+03, 3.4920e+03, 3.4190e+03, 3.3540e+03, 3.2300e+03,\n",
       "        3.1210e+03, 3.0660e+03, 3.0880e+03, 3.0130e+03, 2.8560e+03,\n",
       "        2.7590e+03, 2.7040e+03, 2.6560e+03, 2.6520e+03, 2.4950e+03,\n",
       "        2.3900e+03, 2.4890e+03, 2.3240e+03, 2.2590e+03, 2.1540e+03,\n",
       "        2.1030e+03, 2.0630e+03, 2.0140e+03, 1.9550e+03, 1.8720e+03,\n",
       "        1.8550e+03, 1.7480e+03, 1.7820e+03, 1.6880e+03, 1.6520e+03,\n",
       "        1.6000e+03, 1.5320e+03, 1.5130e+03, 1.3760e+03, 1.4130e+03,\n",
       "        1.3480e+03, 1.3540e+03, 1.3210e+03, 1.2350e+03, 1.2000e+03,\n",
       "        1.1950e+03, 1.0970e+03, 1.0710e+03, 1.0800e+03, 1.0500e+03,\n",
       "        9.9700e+02, 9.4300e+02, 9.3800e+02, 9.5700e+02, 9.6500e+02,\n",
       "        9.0000e+02, 8.7200e+02, 8.1400e+02, 7.6600e+02, 7.6100e+02,\n",
       "        7.3300e+02, 7.1800e+02, 6.8700e+02, 6.6900e+02, 6.4800e+02,\n",
       "        6.6200e+02, 5.9800e+02, 5.9100e+02, 5.8000e+02, 5.4900e+02,\n",
       "        5.3200e+02, 5.1400e+02, 4.7300e+02, 4.9200e+02, 4.7400e+02,\n",
       "        4.0800e+02, 4.4900e+02, 4.1300e+02, 4.1900e+02, 4.1600e+02,\n",
       "        3.8000e+02, 3.7400e+02, 3.9600e+02, 3.3300e+02, 3.2400e+02,\n",
       "        3.5900e+02, 3.3000e+02, 3.2300e+02, 2.7300e+02, 2.7400e+02,\n",
       "        2.7100e+02, 2.4300e+02, 2.5200e+02, 2.5100e+02, 2.3400e+02,\n",
       "        2.3600e+02, 1.9400e+02, 1.9900e+02, 1.8500e+02, 1.9700e+02,\n",
       "        1.7300e+02, 1.8100e+02, 1.6900e+02, 1.6400e+02, 1.5000e+02,\n",
       "        1.4900e+02, 1.2700e+02, 1.3000e+02, 1.3300e+02, 1.1700e+02,\n",
       "        1.2900e+02, 1.0300e+02, 1.0500e+02, 1.0400e+02, 1.1300e+02,\n",
       "        1.2100e+02, 9.7000e+01, 1.0500e+02, 8.2000e+01, 8.4000e+01,\n",
       "        8.1000e+01, 9.0000e+01, 7.7000e+01, 6.8000e+01, 6.9000e+01,\n",
       "        6.5000e+01, 7.6000e+01, 6.3000e+01, 4.6000e+01, 5.0000e+01,\n",
       "        6.1000e+01, 5.7000e+01, 4.6000e+01, 3.8000e+01, 4.7000e+01,\n",
       "        4.3000e+01, 4.9000e+01, 4.2000e+01, 4.1000e+01, 4.2000e+01,\n",
       "        4.5000e+01, 3.5000e+01, 2.2000e+01, 3.3000e+01, 2.9000e+01,\n",
       "        3.3000e+01, 2.3000e+01, 2.9000e+01, 2.1000e+01, 2.0000e+01,\n",
       "        3.1000e+01, 1.9000e+01, 2.3000e+01, 1.6000e+01, 1.6000e+01,\n",
       "        1.5000e+01, 1.2000e+01, 1.6000e+01, 1.3000e+01, 1.4000e+01,\n",
       "        2.3000e+01, 1.5000e+01, 5.0000e+00, 1.0000e+01, 1.6000e+01,\n",
       "        9.0000e+00, 1.2000e+01, 9.0000e+00, 8.0000e+00, 8.0000e+00,\n",
       "        1.0000e+01, 8.0000e+00, 1.1000e+01, 8.0000e+00, 7.0000e+00,\n",
       "        5.0000e+00, 8.0000e+00, 6.0000e+00, 7.0000e+00, 6.0000e+00,\n",
       "        4.0000e+00, 9.0000e+00, 8.0000e+00, 7.0000e+00, 8.0000e+00,\n",
       "        4.0000e+00, 5.0000e+00, 6.0000e+00, 4.0000e+00, 5.0000e+00,\n",
       "        4.0000e+00, 1.0000e+00, 5.0000e+00, 1.0000e+00, 3.0000e+00,\n",
       "        9.0000e+00, 5.0000e+00, 1.0000e+00, 3.0000e+00, 0.0000e+00,\n",
       "        5.0000e+00, 4.0000e+00, 0.0000e+00, 4.0000e+00, 3.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 2.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "        2.0000e+00, 1.0000e+00, 0.0000e+00, 3.0000e+00, 1.0000e+00,\n",
       "        2.0000e+00, 3.0000e+00, 1.0000e+00, 2.0000e+00, 1.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 2.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 1.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00]),\n",
       " array([-0.54880247, -0.54767431, -0.54654614, ...,  0.57710685,\n",
       "         0.57823501,  0.57936318]),\n",
       " <a list of 1000 Patch objects>)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAD8CAYAAACcjGjIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAEp1JREFUeJzt3X+sZGV9x/H3111XbRRZ4IJ0F3sxLo1IUtQb3MYYWxBYtBH+wHaNlrUh3UQwsbE/XGsTUq0JtKm0RLSlQlxMLaC1shEsrvyIbcPCXopCF4J7QSu3EFi7QCVGFP32j3mune6Zu3Nm7tw58+P9SiZzznOemXmenbnnc57nnJmNzESSpHYvaLoBkqTRYzhIkioMB0lSheEgSaowHCRJFYaDJKnCcJAkVRgOkqQKw0GSVLG26Qb065hjjsnZ2dmmmyFJY+Oee+75fmbO1Kk7tuEwOzvL/Px8082QpLEREf9Zt67TSpKkCsNBklRhOEiSKgwHSVKF4SBJqjAcJEkVhoMkqcJwkCRVGA6SpArDQerD7I6bmm6CtKoMB2kAlsLC0NCkMBykPhkImmRj+8N7UhMODYL2dUNCk8SRgzRghoQmgeEg1eROX9PEcJBWgUGicWc4SDW4s9e0MRykVTK74yZDRWPLcJC6cAevaWQ4SJIqDAdplTny0DgyHKTDcMeuaWU4SMswGDTNDAdJUoXhIHUw6FGDoxCNG8NBklRhOEhD4uhB48RwkCRVGA7SEDl60LioHQ4RsSYi7o2Ir5T1EyPirojYHxHXR8S6Uv6isr5Qts+2PceHS/lDEXF2W/mWUrYQETsG1z2pd+7Apd5GDh8AHmxbvwy4PDM3AU8BF5byC4GnMvPVwOWlHhFxMrAVeC2wBfhUCZw1wJXAOcDJwLtKXUlSQ2qFQ0RsBN4OfKasB3A68MVSZSdwXlk+t6xTtp9R6p8LXJeZz2Xmd4AF4LRyW8jMRzLzx8B1pa40dI4apJa6I4e/Av4I+FlZPxp4OjOfL+uLwIayvAF4FKBsf6bU/3n5IY9ZrlyaSAaQxkHXcIiI3wCezMx72os7VM0u23ot79SW7RExHxHzBw4cOEyrJUkrUWfk8CbgHRHxXVpTPqfTGkkcGRFrS52NwGNleRE4AaBsfzlwsL38kMcsV16RmVdl5lxmzs3MzNRoulTfMI/oHT1o1HUNh8z8cGZuzMxZWieUb8vMdwO3A+eXatuAG8vyrrJO2X5bZmYp31quZjoR2ATcDewFNpWrn9aV19g1kN5JkvqytnuVZX0IuC4i/gy4F7i6lF8NfC4iFmiNGLYCZOa+iLgBeAB4Hrg4M38KEBHvB24B1gDXZOa+FbRLkrRCPYVDZt4B3FGWH6F1pdGhdX4EvHOZx38c+HiH8puBm3tpiyRp9fgNaYlmzgF43kGjzHCQJFUYDpKkCsNBU6/J6R2nljSqDAdJUoXhIEmqMBw01ZzWkTozHCRJFYaDJKnCcJAa5tSWRpHhIEmqMBw0tUbpiH2U2iKB4SBJ6sBwkCRVGA6aSk7jSIdnOEiSKgwHSVKF4SCNCKe6NEoMB0lSheGgqTPKR+ij3DZNF8NBklRhOEiSKgwHSVKF4aCp4py+VI/hIEmqMBykEePoRqPAcJAkVRgOmhoekUv1GQ7SCDLI1DTDQZJUYThIkioMB00Fp2mk3hgO0ogy0NQkw0GSVGE4SJIqDAdJUoXhIEmq6BoOEfHiiLg7Ir4VEfsi4k9L+YkRcVdE7I+I6yNiXSl/UVlfKNtn257rw6X8oYg4u618SylbiIgdg++mppkndqXe1Rk5PAecnpm/ApwKbImIzcBlwOWZuQl4Criw1L8QeCozXw1cXuoREScDW4HXAluAT0XEmohYA1wJnAOcDLyr1JUkNaRrOGTLs2X1heWWwOnAF0v5TuC8snxuWadsPyMiopRfl5nPZeZ3gAXgtHJbyMxHMvPHwHWlrjT1HPWoKbXOOZQj/G8CTwK7gYeBpzPz+VJlEdhQljcAjwKU7c8AR7eXH/KY5cqlFXPnKvWnVjhk5k8z81RgI60j/dd0qlbuY5ltvZZXRMT2iJiPiPkDBw50b7gkqS89Xa2UmU8DdwCbgSMjYm3ZtBF4rCwvAicAlO0vBw62lx/ymOXKO73+VZk5l5lzMzMzvTRdktSDOlcrzUTEkWX5JcBbgQeB24HzS7VtwI1leVdZp2y/LTOzlG8tVzOdCGwC7gb2ApvK1U/raJ203jWIzkmS+lNn5HA8cHtE3EdrR747M78CfAj4YEQs0DqncHWpfzVwdCn/ILADIDP3ATcADwD/DFxcpqueB94P3EIrdG4odSXheRM1I1oH9eNnbm4u5+fnm26GRtgk7VS/e+nbm26CJkBE3JOZc3Xq+g1paQxMUtBpPBgOkqQKw0GSVGE4SJIqDAdJUoXhoInkCVxpZQwHSVKF4SCNCUdDGibDQRPHnai0coaDJKnCcJAkVRgO0hhxykzDYjhIkioMB00Uj6ylwTAcJEkVhoMmxrSMGqaln2qW4SBJqjAcJEkVhoMkqcJwkMaQ5x202gwHSVKF4SBJqjAcNBGcZpEGy3CQJFUYDtKYcrSk1WQ4SJIqDAdJUoXhoLHn9Io0eIaDNMYMRq0Ww0GSVGE4SJIqDAdpzDm1pNVgOEiSKgwHjTWPmqXVYThIkioMB0lSheEgSaroGg4RcUJE3B4RD0bEvoj4QCk/KiJ2R8T+cr++lEdEXBERCxFxX0S8vu25tpX6+yNiW1v5GyLi/vKYKyIiVqOzmiyeb5BWT52Rw/PA72fma4DNwMURcTKwA7g1MzcBt5Z1gHOATeW2Hfg0tMIEuAR4I3AacMlSoJQ629set2XlXZOmh0GpQesaDpn5eGb+e1n+AfAgsAE4F9hZqu0EzivL5wLXZsse4MiIOB44G9idmQcz8ylgN7ClbDsiM+/MzASubXsuSVIDejrnEBGzwOuAu4DjMvNxaAUIcGyptgF4tO1hi6XscOWLHco7vf72iJiPiPkDBw700nRJUg9qh0NEvBT4R+D3MvN/Dle1Q1n2UV4tzLwqM+cyc25mZqZbkzXBnEaRVletcIiIF9IKhr/PzC+V4ifKlBDl/slSvgic0PbwjcBjXco3diiX1AMDU4NU52qlAK4GHszMT7Rt2gUsXXG0DbixrfyCctXSZuCZMu10C3BWRKwvJ6LPAm4p234QEZvLa13Q9lySpAasrVHnTcBvA/dHxDdL2R8DlwI3RMSFwPeAd5ZtNwNvAxaAHwK/A5CZByPiY8DeUu+jmXmwLL8P+CzwEuCr5SZJakjXcMjMf6XzeQGAMzrUT+DiZZ7rGuCaDuXzwCnd2iJJGg6/Ia2x49z68vy30aAYDpKkCsNBklRhOGisOG0iDYfhIEmqMBykCePoSoNgOEiSKgwHSVKF4aCx4XSJNDyGgySpwnCQJpCjLK2U4aCx4M5OGi7DQZpQBqpWwnCQJFUYDhp5HgFLw2c4SJIqDAdpgjnqUr8MB0lSheEgSaowHCRJFYaDRppz5ivnv6H6YThIkioMB40sj3il5hgOkqQKw0GaAo7C1CvDQSPJnZnULMNBklRhOEhTwtGYemE4SJIqDAeNHI9wpeYZDtIUMXhVl+EgSaowHCRJFYaDJKnCcNBIcU5cGg2GgzRlDGDV0TUcIuKaiHgyIv6jreyoiNgdEfvL/fpSHhFxRUQsRMR9EfH6tsdsK/X3R8S2tvI3RMT95TFXREQMupMaD+60pNFRZ+TwWWDLIWU7gFszcxNwa1kHOAfYVG7bgU9DK0yAS4A3AqcBlywFSqmzve1xh76WpAEziNVN13DIzG8ABw8pPhfYWZZ3Aue1lV+bLXuAIyPieOBsYHdmHszMp4DdwJay7YjMvDMzE7i27bkkSQ3p95zDcZn5OEC5P7aUbwAebau3WMoOV77YoVxTxiPZ4fPfXIcz6BPSnc4XZB/lnZ88YntEzEfE/IEDB/psoiSpm37D4YkyJUS5f7KULwIntNXbCDzWpXxjh/KOMvOqzJzLzLmZmZk+my5J6qbfcNgFLF1xtA24sa38gnLV0mbgmTLtdAtwVkSsLyeizwJuKdt+EBGby1VKF7Q9l6aE0xvS6KlzKes/AHcCvxwRixFxIXApcGZE7AfOLOsANwOPAAvA3wEXAWTmQeBjwN5y+2gpA3gf8JnymIeBrw6ma5K6MZi1nLXdKmTmu5bZdEaHuglcvMzzXANc06F8HjilWzs0mdw5SaPJb0hLkioMB2nKOXpTJ4aDGuNOSRpdhoMkg1oVhoMa4c5IGm2GgyTAwNb/ZzhIkioMB0lSheGgoXP6YnT53miJ4SBJqjAcNFQemY4+3yOB4aAhcqcjjQ/DQVKFQS7DQZJUYThoKDwSlcaL4SCpIwN9uhkOWnXuZMaX7930Mhy0qty5SOPJcJB0WAb8dDIctGrcqUjjy3CQ1JVBP30MB60KdybSeDMcNHAGw2TyfZ0uhoOk2gyI6WE4aKDceUw+3+PpYDhoYNxpSJPDcJDUMw8EJp/hoIFwZzF9fM8nm+GgFXMnMb187yeX4aAVcecgPwOTyXBQ39wpaImfhcljOKhnsztucmegCj8Tk8VwkDQwBsTkWNt0AzQ+/MNXHUufk+9e+vaGW6KVMBzUlaEgTR+nlbQsQ0ErsXRuys/ReIrMbLoNfZmbm8v5+fmmmzGx/IPWanG6qTkRcU9mztWpOzIjh4jYEhEPRcRCROxouj3TyiM9rTY/X+NhJM45RMQa4ErgTGAR2BsRuzLzgWZbNvn8Q1UTDv3cOZoYPSMRDsBpwEJmPgIQEdcB5wKGw4AYAhplnT6fBkazRiUcNgCPtq0vAm9sqC1jxZ2+JlUvn22DZPBGJRyiQ1nlTHlEbAe2l9VnI+K/ge+vZsMacgz2a5zYr4bFZT1VH5t+9ahOv36p7pONSjgsAie0rW8EHju0UmZeBVy1tB4R83XPvI8T+zVe7Nd4sV/1jMrVSnuBTRFxYkSsA7YCuxpukyRNrZEYOWTm8xHxfuAWYA1wTWbua7hZkjS1RiIcADLzZuDmHh92VfcqY8l+jRf7NV7sVw1j+w1pSdLqGZVzDpKkETJW4RARR0XE7ojYX+7XL1PvlRHxtYh4MCIeiIjZ4ba0N3X7VeoeERH/FRGfHGYb+1GnXxFxakTcGRH7IuK+iPitJtpaR7efeImIF0XE9WX7XaP+uYNaffpg+Ru6LyJujYjal0I2re5P8kTE+RGRETHyVzDV6VNE/GZ5z/ZFxOf7frHMHJsb8OfAjrK8A7hsmXp3AGeW5ZcCv9B02wfRr7L9r4HPA59sut2D6BdwErCpLP8i8DhwZNNt79DONcDDwKuAdcC3gJMPqXMR8DdleStwfdPtHkCffn3p7wd436j3qZe+lXovA74B7AHmmm73AN6vTcC9wPqyfmy/rzdWIwdaP6mxsyzvBM47tEJEnAyszczdAJn5bGb+cHhN7EvXfgFExBuA44CvDaldK9W1X5n57czcX5YfA54EZobWwvp+/hMvmfljYOknXtq19/eLwBkR0ekLnqOia58y8/a2v589tL6DNA7qvF8AH6N1EPOjYTauT3X69LvAlZn5FEBmPtnvi41bOByXmY8DlPtjO9Q5CXg6Ir4UEfdGxF+UH/YbZV37FREvAP4S+MMht20l6rxfPxcRp9E6Inp4CG3rVaefeNmwXJ3MfB54Bjh6KK3rT50+tbsQ+OqqtmhwuvYtIl4HnJCZXxlmw1agzvt1EnBSRPxbROyJiC39vtjIXMq6JCK+Dryiw6aP1HyKtcCbgdcB3wOuB94LXD2I9vVrAP26CLg5Mx8dpYPRAfRr6XmOBz4HbMvMnw2ibQNW5ydeav0MzAip3d6IeA8wB7xlVVs0OIftWznYupzWvmFc1Hm/1tKaWvo1WqO8f4mIUzLz6V5fbOTCITPfuty2iHgiIo7PzMfLzqTTkGkRuDf/7xdevwxspuFwGEC/fhV4c0RcROs8yrqIeDYzG/2/LwbQLyLiCOAm4E8yc88qNXWl6vzEy1KdxYhYC7wcODic5vWl1s/WRMRbaYX9WzLzuSG1baW69e1lwCnAHeVg6xXAroh4R2aO6v8iVvczuCczfwJ8JyIeohUWe3t9sXGbVtoFbCvL24AbO9TZC6yPiKV569MZ/Z/+7tqvzHx3Zr4yM2eBPwCubToYaujar/JzKf9Eqz9fGGLbelXnJ17a+3s+cFuWs4IjqmufytTL3wLvWMn8dQMO27fMfCYzj8nM2fI3tYdWH0c1GKDeZ/DLtC4iICKOoTXN9Ehfr9b0Gfgez9YfDdwK7C/3R5XyOeAzbfXOBO4D7gc+C6xruu2D6Fdb/fcyHlcrde0X8B7gJ8A3226nNt32ZfrzNuDbtM6JfKSUfZTWTgXgxcAXgAXgbuBVTbd5AH36OvBE23uzq+k2D6pvh9S9gxG/Wqnm+xXAJ2gdEN8PbO33tfyGtCSpYtymlSRJQ2A4SJIqDAdJUoXhIEmqMBwkSRWGgySpwnCQJFUYDpKkiv8FnCgWq5BezzEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "samples = 10000000\n",
    "nrml_data_set = np.random.normal(0, 0.1, samples)\n",
    "print(np.mean(nrml_data_set))\n",
    "print(np.median(nrml_data_set))\n",
    "plt.hist(nrml_data_set, bins=1000)\n",
    "\n",
    "# scan the documentation for the `mean` and `median` functions calculate the mean and median\n",
    "\n",
    "# Do you notice anything about the mean and the median?\n",
    "\n",
    "# Using the np.random.normal function, generate a few datasets and compare the median and mean\n",
    "# for each dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variance of a dataset measure how **spread apart** the data are from eachother.\n",
    "\n",
    "Which histograms do you think display _high variance_, which have _low variance_?\n",
    "![variance example](https://qph.fs.quoracdn.net/main-qimg-e58c4ef2d7591f651e6f6e19d8550fae)\n",
    "\n",
    "Formally, we calculate the variance as the _normalized sum of the squares of each data's magnitude from the mean_. In other words, we:\n",
    "    1. look at every data point\n",
    "    1. calculate how far away it is from the mean\n",
    "    1. square each difference and sum them all together\n",
    "    1. normalize by the total number of data points\n",
    "    \n",
    "$$\n",
    "\\sigma^2 = \\frac{\\sum{(x_i - \\overline{x})^2}}{n}\n",
    "$$\n",
    "\n",
    "> **Notice** that the variance is in squared units. People will often take the square root of the variance in order to have a quantity with the same unit as the mean. We call this value the _standard deviation_.\n",
    "\n",
    "$$\n",
    "\\text{SD}  = \\sqrt{\\sigma^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use another library called pandas to load in our data.\n",
    "# Pandas is built on top of numpy and supports it's functionality\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Id',\n",
       " 'EmployeeName',\n",
       " 'JobTitle',\n",
       " 'BasePay',\n",
       " 'OvertimePay',\n",
       " 'OtherPay',\n",
       " 'Benefits',\n",
       " 'TotalPay',\n",
       " 'TotalPayBenefits',\n",
       " 'Year',\n",
       " 'Notes',\n",
       " 'Agency',\n",
       " 'Status']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "salary_data = pd.read_csv('Salaries.csv', low_memory=False)\n",
    "\n",
    "# Examine the data\n",
    "list(salary_data.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the TotalPay column into an array so we may analyze it\n",
    "total_pay = salary_data.loc[:,\"TotalPay\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    148654.000000\n",
       "mean      74768.321972\n",
       "std       50517.005274\n",
       "min        -618.130000\n",
       "25%       36168.995000\n",
       "50%       71426.610000\n",
       "75%      105839.135000\n",
       "max      567595.430000\n",
       "Name: TotalPay, dtype: float64"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate the mean, median, variance and standard deviation of\n",
    "# the total pay and base pay data sets\n",
    "np.std(total_pay)\n",
    "total_pay.describe()\n",
    "\n",
    "## WORK HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias\n",
    "\n",
    "In reality, whenever we collect data, there is an inherent randomness. If you repeat an experiment many times, you may get slightly different data every time. The error will not be corrected if we average out these results, and we call this error **bias**.\n",
    "\n",
    "\n",
    "**In other words:**\n",
    "\n",
    "_Bias_ is what happens when we fail to take information into account or have incorrect assumptions. \n",
    "\n",
    "> Discussion time. Can you think of examples of how bias may have affected our salary data set above?\n",
    "\n",
    "### Overfitting and Underfitting\n",
    "\n",
    "When we build models, we want to build models which are specific enough that they yield relevant predictions, but also general enough that they can be applied to new an unseen datasets.\n",
    "\n",
    "If a model is too specific to the data, we are unable to generalize the results, and we call this **overfitting**\n",
    "\n",
    "If a model is not specific enough to yield meaningful predictions, we call this **underfitting**\n",
    "\n",
    "> regression models\n",
    "\n",
    "![fit curves](https://qph.ec.quoracdn.net/main-qimg-b4112b5d856f4f0da349460aeed854d8)\n",
    "\n",
    "> classification models\n",
    "\n",
    "![more fit curves](https://cdncontribute.geeksforgeeks.org/wp-content/uploads/fittings.jpg)\n",
    "\n",
    "> Discussion time.\n",
    "\n",
    "    - Why is underfitting and overfitting undesirable?\n",
    "    - Can you think of ways that we may inadvertently overfit data?\n",
    "    - How might we underfit data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias Variance Tradeoff\n",
    "\n",
    "> IMPORTANT!!\n",
    "\n",
    "There is a fundamental property which states that error of a model is directly related to the sum of the **variance** and the **bias**^2.\n",
    "\n",
    "![variance tradeoff](https://www.researchgate.net/profile/Ljubomir_Jacic2/post/How_does_model_complexity_impact_the_bias-variance_tradeoff/attachment/59d6233579197b807798188f/AS%3A306150770184192%401450003439733/image/biasvariance.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
